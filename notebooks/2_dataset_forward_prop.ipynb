{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a dataset and  forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by generating a dataset. Scikit-learn is a very nice tool for machine learning. It includes several dataset. We will use one of them. Then we will define the size of our neurql network, the activation function and see how foward propagation will work exactly in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# magic command to plot inside the notebook\n",
    "%matplotlib inline \n",
    "# you can customize the plot size\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "\n",
    "# The seed allow us to predict random numbers, it's easier to debug\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.10)\n",
    "\n",
    "# The dataset is labeled by 0 and 1\n",
    "# So we can plot by using matplotlib\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a nice dataset which we can use for our neural network. The job is to classify those data. In other words, your NN has to seperate both population of points. You can change the noise as you want, increasing it will make the classification more challenging for the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN characteristics, loss function and feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the dataset, let's talk about what neural network we will code.  \n",
    "First, we have to predict 2 categories (blue and red, male and female, 0 and 1, etc). So we need at least 1 output. We will code **2 outputs** in order to re-use our NN later if necessary.  \n",
    "About the inputs, we have $x$ and $y$ so we will have **2 inputs**.  \n",
    "In-between, we will have what we call hidden layer(s). The more we have, the more we can approximate our function. But let's make it simple, we will use **1 hidden layer** and will make customizable the number of neurons in this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Our Neural Network](images/NN_basic.png \"Our Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose our activation function. This function will take an input and tranform it through this function to give and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Activation function](images/activation_function.png \"Activation function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more simple, we will use **tanh** (c), hyperbolic tangent. It's simpler because later, we will have to derive it and $$(tanh(x))'=\\frac{1}{cosh(x)^2}=1+tanh(x)^2$$  \n",
    "Maybe you cannot believe me now but it's convinient. In term of computation, you can re-use $tanh$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FeedForward](images/nn_ff_1.png \"FeedForward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this configuration, we have 6 matrices to manage:  \n",
    "$x = \\begin{bmatrix}\n",
    "    x_1 \\\\ \n",
    "    x_2 \\\\\n",
    "\\end{bmatrix}, \\quad \n",
    "W_{layer 1} = \\begin{bmatrix}\n",
    "    W_{1,1} & W_{1,2} \\\\\n",
    "    W_{2,1} & W_{2,2} \\\\\n",
    "    W_{3,1} & W_{3,2} \\\\\n",
    "    W_{4,1} & W_{4,2} \\\\\n",
    "    W_{5,1} & W_{5,2} \\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "b_{layer 1} = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3 \\\\\n",
    "    b_4 \\\\\n",
    "    b_5 \\\\\n",
    "\\end{bmatrix}\n",
    "$. For the input of the first layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{layer 2} = \\begin{bmatrix}\n",
    "    W_{1,1} & W_{1,2} & W_{1,3} & W_{1,4} & W_{1,5} \\\\\n",
    "    W_{2,1} & W_{2,2} & W_{2,3} & W_{2,4} & W_{2,5}\\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "b_{layer 2} = \\begin{bmatrix}\n",
    "    b_1 & b_2 & b_3 & b_4 & b_5\\\\\n",
    "\\end{bmatrix}\n",
    "$. For the input of the last layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally:  \n",
    "$\n",
    "\\widehat Y = \\begin{bmatrix}\n",
    "    y_1 \\\\ \n",
    "    y_2\\\\\n",
    "\\end{bmatrix}\n",
    "$  \n",
    "Where $x$ is the inputs ($x$,$y$ on the dataset figure), $W$ is the weights, $b$ the biases and $\\widehat Y$ the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to compute those input into the next layer. To be more concrete, we have to compute:  \n",
    "$$i_1 =x\\quad \\times \\quad W_{layer 1}\\quad+\\quad b_{layer 1}$$  \n",
    "$$h_1 = \\tanh(i_1)$$  \n",
    "$$i_2=x\\quad \\times \\quad W_{layer 2}\\quad+\\quad b_{layer 2}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is what we call in mathematics Linear Algebra. It's matrix calculation. GPUs are specialy desgined for this kind of operations. But in our example, we will use our CPU.  \n",
    "The matrices **W and b are called the model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have almost all theoritical inforamtion. Let's write some code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's initilize the parameters we will need later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples = len(X) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "nn_hdim = 5 # nb of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize our model by putting random values. This part is actually an important topic. We will not go into details but you should read about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters to random values. We need to learn these.\n",
    "\n",
    "# The seed make the random predictable\n",
    "# So it's easier to debug\n",
    "np.random.seed(0)\n",
    "\n",
    "# We are creating matrices with the corresponding size\n",
    "W1 = np.random.randn(nn_input_dim, nn_hdim) \n",
    "b1 = np.zeros((1, nn_hdim))\n",
    "W2 = np.random.randn(nn_hdim, nn_output_dim) \n",
    "b2 = np.zeros((1, nn_output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to store our model somewhere, so we will use a python dictionnary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(model):\n",
    "    # Compute the feedforward\n",
    "    \n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    i1 = X.dot(W1) + b1\n",
    "    h1 = np.tanh(i1)\n",
    "    i2 = h1.dot(W2) + b2\n",
    "    \n",
    "    return i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feedforward(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output is not very useful. So we have to add a new function into the last layer.  This function is called softmax function. To be simple, it's converting our strange input into probabilities (between 0 and 1). Then we will take the higher probabilities from both output and we will have the classification of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ output = \\widehat{y} = softmax(i_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(model, X):\n",
    "    # Compute the feedforward\n",
    "    \n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    i1 = X.dot(W1) + b1\n",
    "    h1 = np.tanh(i1)\n",
    "    i2 = h1.dot(W2) + b2\n",
    "    \n",
    "    # the softmax part\n",
    "    exp_scores = np.exp(i2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = feedforward(model, X)\n",
    "print('The size of our samples is',len(X))\n",
    "print('And the size of our input:',len(output))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: feedforward(model, x))\n",
    "plt.title(\"Our non-yet-trained model is not very useful...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our model is very bad! Actually, we initialized it with random values so it was expected... In the next part, we will learn to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
