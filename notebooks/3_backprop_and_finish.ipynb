{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and analysing our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, if you're no so confident about why we have to optimize our loss function, please go back to 1_fct_approx.ipynb.  \n",
    "You can change ar and br by hand in order to feet our line on the dataset as much as possible. Mathematically you have to decrease the average distance between the data and you line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an example by hand:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BackProp](images/backprop_example.png \"Back Propagation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case:   \n",
    "$f(Input,W,b)=(Input+W)b$  \n",
    "$q = Input+W$  \n",
    "$f = qb$  \n",
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = -15\n",
    "W = -6\n",
    "b = -2\n",
    "q = -15 + -6\n",
    "f = q*b\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to compute:  \n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial Input},\\frac{\\partial f}{\\partial W}, \\frac{\\partial f}{\\partial b}\n",
    "\\end{equation}\n",
    "And for information:\n",
    "$$\\frac{\\partial q}{\\partial Input}=1, \\frac{\\partial q}{\\partial W}=1$$  \n",
    "$$\\frac{\\partial f}{\\partial q}=b, \\frac{\\partial f}{\\partial b}=q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the output, we have $\\frac{\\partial f}{\\partial f}=1$. Then $\\frac{\\partial f}{\\partial b}=-21$, $\\frac{\\partial f}{\\partial q}=-2$ and $\\frac{\\partial q}{\\partial W}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BackProp](images/backprop_example_comp.png \"Back Propagation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In term of interpretation, the value of the gradient is indicating the influence of this value on the output.  \n",
    "If we are increasing b, it will decrease a lot the output for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go back to our own neural network. For reminder:  \n",
    "![OurNN](images/NN_basic.png \"Our NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$i_1 =x\\quad \\times \\quad W_{layer 1}\\quad+\\quad b_{layer 1}$$  \n",
    "$$h_1 = \\tanh(i_1)$$  \n",
    "$$i_2=x\\quad \\times \\quad W_{layer 2}\\quad+\\quad b_{layer 2}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our variables are:  \n",
    "$W_{layer 1} = \\begin{bmatrix}\n",
    "    W_{1,1} & W_{1,2} \\\\\n",
    "    W_{2,1} & W_{2,2} \\\\\n",
    "    W_{3,1} & W_{3,2} \\\\\n",
    "    W_{4,1} & W_{4,2} \\\\\n",
    "    W_{5,1} & W_{5,2} \\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "b_{layer 1} = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3 \\\\\n",
    "    b_4 \\\\\n",
    "    b_5 \\\\\n",
    "\\end{bmatrix},\n",
    "W_{layer 2} = \\begin{bmatrix}\n",
    "    W_{1,1} & W_{1,2} & W_{1,3} & W_{1,4} & W_{1,5} \\\\\n",
    "    W_{2,1} & W_{2,2} & W_{2,3} & W_{2,4} & W_{2,5}\\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "b_{layer 2} = \\begin{bmatrix}\n",
    "    b_1 & b_2 & b_3 & b_4 & b_5\\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our loss function is:  $$L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\widehat{y}_{n,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to analyse the behavior of our loss function compare to all our varibables in order to decrease her (the loss function) value. So we wil compute the derivation for each vector $W$ and $b$.  \n",
    "It means we want: \n",
    "$$\\frac{\\partial L}{\\partial W_1}, \\frac{\\partial L}{\\partial b_1}, \\frac{\\partial L}{\\partial W_2}, \\frac{\\partial L}{\\partial b_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it simpler, I'm giving you the gradient directly:  \n",
    "$$\\delta_3=\\widehat{y}-y$$\n",
    "$$\\delta_2=(1-\\tanh^2i_2).\\delta_3W^T_2$$\n",
    "$$\\frac{\\partial L}{\\partial W_2}=a_1^T.\\delta_3$$\n",
    "$$\\frac{\\partial L}{\\partial b_2}=\\delta_3$$\n",
    "$$\\frac{\\partial L}{\\partial W_1}=x^T.\\delta_2$$\n",
    "$$\\frac{\\partial L}{\\partial b_1}=\\delta_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need, we can implement our neural network completely! First we have to import libraries and load our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is from Denny Britz. You can find his github there: https://github.com/dennybritz and his awesome blog wildml.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# magic command to plot inside the notebook\n",
    "%matplotlib inline \n",
    "# you can customize the plot size\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "\n",
    "# The seed allow us to predict random numbers, it's easier to debug\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.10)\n",
    "\n",
    "# The dataset is labeled by 0 and 1\n",
    "# So we can plot by using matplotlib\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples = len(X) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the FeedForward part we did before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to evaluate our model. In order to do this, we have to create a function that calculating the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the training part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(nn_hdim, num_passes=4000, print_loss=False):\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 100 == 0:\n",
    "          print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! Let's try our code!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = build_model(3, print_loss=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: predict(model, x))\n",
    "plt.title(\"Decision Boundary for hidden layer size 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our NN classified our dataset quite well. depends on the noise. \n",
    "Now we can try different size of hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\n",
    "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer size %d' % nn_hdim)\n",
    "    model = build_model(nn_hdim)\n",
    "    plot_decision_boundary(lambda x: predict(model, x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
